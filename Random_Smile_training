import os
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from rdkit import Chem
from molvs import Standardizer, LargestFragmentChooser, ValenceError

#  SETTINGS & HYPERPARAMETERS
SMILES_CSV      = "smiles_for_sdvae.csv"       # original CSV with column "SMILES"
CHECKPOINT_PATH = "random_vae.pth"
GENERATED_OUT   = "generated_smiles_random.txt"
REFERENCE_PATH  = "smiles_for_sdvae.txt"       # text file with one canonical SMILES per line

EMBED_DIM       = 128
HIDDEN_DIM      = 256
LATENT_DIM      = 128
NUM_LAYERS      = 2
LEARNING_RATE   = 1e-3
BATCH_SIZE      = 64
NUM_EPOCHS      = 50
TEACHER_FORCING = 0.5    # 50% teacher forcing
MAX_LEN         = 100    # maximum SMILES length (including <bos>/<eos>)
TEMPERATURE     = 0.8
SAMPLE_COUNT    = 100

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#  PREPROCESSING FUNCTIONS
_standardizer = Standardizer()
_largest = LargestFragmentChooser()

def is_valid_strict(smiles: str) -> bool:
    """
    Use RDKit + MolVS to check valence, aromaticity, strip salts, normalize charges.
    Returns True if SMILES is chemically valid.
    """
    try:
        mol = Chem.MolFromSmiles(smiles, sanitize=False)
        if mol is None:
            return False
        mol = _largest.choose(mol)               # keep largest fragment
        mol = _standardizer.standardize(mol)     # standardize charges/isotopes
        Chem.SanitizeMol(mol)                    # full RDKit sanitisation
        return True
    except (ValenceError, ValueError, Chem.rdchem.KekulizeException):
        return False

def sanitise_and_canonicalise(smiles: str) -> str | None:
    """
    If valid, return the canonical isomeric SMILES; else return None.
    """
    if not is_valid_strict(smiles):
        return None
    mol = Chem.MolFromSmiles(smiles, sanitize=False)
    if mol is None:
        return None
    mol = _largest.choose(mol)
    mol = _standardizer.standardize(mol)
    Chem.SanitizeMol(mol)
    return Chem.MolToSmiles(mol, canonical=True, isomericSmiles=True)

def enumerate_randomised(smi: str, n: int=10, seed: int|None=None) -> list[str]:
    """
    Optional augmentation: generate n randomised SMILES for a single canonical SMILES.
    Set n=1 (or 0) to skip augmentation.
    """
    if seed is not None:
        random.seed(seed)
    mol = Chem.MolFromSmiles(smi)
    if mol is None:
        return []
    out = set()
    for _ in range(n):
        out.add(Chem.MolToSmiles(mol, doRandom=True, canonical=False))
    return list(out)

#  VOCABULARY CONSTRUCTION
def build_vocab(smiles_list: list[str]) -> tuple[dict[str,int], list[str]]:
    """
    Build stoi/itos given a list of (canonical) SMILES.
    Special tokens: <pad>=0, <bos>=1, <eos>=2, <unk>=3
    The rest are sorted unique characters in the SMILES corpus.
    """
    specials = ['<pad>', '<bos>', '<eos>', '<unk>']
    charset = set()
    for smi in smiles_list:
        charset.update(list(smi))
    itos = specials + sorted(charset)
    stoi = {ch:i for i,ch in enumerate(itos)}
    return stoi, itos

#  DATASET & DATALOADER
class SmilesDataset(Dataset):
    def __init__(self, smiles: list[str], stoi: dict[str,int], max_len: int):
        self.smiles = smiles
        self.stoi = stoi
        self.max_len = max_len

    def __len__(self):
        return len(self.smiles)

    def __getitem__(self, idx: int):
        smi = self.smiles[idx]
        # Add <bos> and <eos>, then pad/truncate to max_len
        tokens = ['<bos>'] + list(smi) + ['<eos>']
        if len(tokens) < self.max_len:
            tokens += ['<pad>'] * (self.max_len - len(tokens))
        else:
            tokens = tokens[:self.max_len]
            tokens[-1] = '<eos>'
        # input = tokens[:-1], target = tokens[1:]
        inp = [self.stoi.get(tok, self.stoi['<unk>']) for tok in tokens[:-1]]
        tgt = [self.stoi.get(tok, self.stoi['<unk>']) for tok in tokens[1:]]
        return torch.tensor(inp, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)

#  VAE MODEL DEFINITION
class Encoder(nn.Module):
    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, latent_dim: int, num_layers: int):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)
        self.fc_mu = nn.Linear(hidden_dim*2, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim*2, latent_dim)

    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        emb = self.embedding(x)                           # [B, L, E]
        _, (h_n, _) = self.lstm(emb)                      # h_n: [2*num_layers, B, hidden_dim]
        # Take last forward and backward, concatenate:
        h_for = h_n[-2]   # (last layer, forward)
        h_back = h_n[-1]  # (last layer, backward)
        h = torch.cat([h_for, h_back], dim=-1)            # [B, hidden_dim*2]
        mu = self.fc_mu(h)                                # [B, latent_dim]
        logvar = self.fc_logvar(h)                        # [B, latent_dim]
        return mu, logvar

class Decoder(nn.Module):
    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, latent_dim: int, num_layers: int, stoi: dict[str,int]):
        super().__init__()
        self.stoi = stoi
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.fc_z2hidden = nn.Linear(latent_dim, hidden_dim * num_layers)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)
        self.output = nn.Linear(hidden_dim, vocab_size)

    def forward(self, z: torch.Tensor, target: torch.Tensor|None, teacher_forcing_ratio: float) -> torch.Tensor:
        B = z.size(0)
        device = z.device
        # initialize hidden states from z
        h = torch.tanh(self.fc_z2hidden(z))                # [B, hidden_dim * num_layers]
        num_layers = self.lstm.num_layers
        hidden = h.view(num_layers, B, self.lstm.hidden_size)
        cell = torch.zeros_like(hidden).to(device)

        # prepare to decode
        seq_len = target.size(1) if target is not None else MAX_LEN - 1
        input_idx = torch.full((B,), self.stoi['<bos>'], device=device, dtype=torch.long)

        logits_list = []
        for t in range(seq_len):
            emb = self.embedding(input_idx).unsqueeze(1)    # [B, 1, E]
            out, (hidden, cell) = self.lstm(emb, (hidden, cell))
            logit = self.output(out.squeeze(1))             # [B, vocab_size]
            logits_list.append(logit.unsqueeze(1))          # accumulate

            if target is not None and random.random() < teacher_forcing_ratio:
                input_idx = target[:, t]
            else:
                probs = torch.softmax(logit / TEMPERATURE, dim=-1)
                input_idx = torch.multinomial(probs, 1).squeeze(-1)

        return torch.cat(logits_list, dim=1)                # [B, seq_len, vocab_size]

class VAE(nn.Module):
    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, latent_dim: int, num_layers: int, stoi: dict[str,int]):
        super().__init__()
        self.encoder = Encoder(vocab_size, embed_dim, hidden_dim, latent_dim, num_layers)
        self.decoder = Decoder(vocab_size, embed_dim, hidden_dim, latent_dim, num_layers, stoi)

    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:
        std = (0.5 * logvar).exp()
        eps = torch.randn_like(std)
        return mu + std * eps

    def forward(self, x: torch.Tensor, target: torch.Tensor|None, teacher_forcing_ratio: float) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        mu, logvar = self.encoder(x)
        z = self.reparameterize(mu, logvar)
        recon_logits = self.decoder(z, target, teacher_forcing_ratio)
        return recon_logits, mu, logvar

def loss_function(recon_logits: torch.Tensor, target: torch.Tensor, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:
    B, T, V = recon_logits.size()
    recon_loss = F.cross_entropy(recon_logits.view(-1, V), target.view(-1), ignore_index=0)
    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_loss + kl

#  MAIN TRAINING PIPELINE
def main():
    # 1) Load CSV, sanitize & canonicalise
    df = pd.read_csv(SMILES_CSV)
    raw_smiles = df["SMILES"].astype(str).tolist()

    # Strict sanitisation
    canon_smiles = []
    for smi in raw_smiles:
        cs = sanitise_and_canonicalise(smi)
        if cs:
            canon_smiles.append(cs)
    canon_smiles = sorted(set(canon_smiles))

    # Optional: randomise each SMILES 5× (set to 1× to skip)
    augmented = []
    for smi in canon_smiles:
        variants = enumerate_randomised(smi, n=5, seed=42)
        augmented.extend(variants or [smi])
    # Final SMILES list used for training
    train_smiles = sorted(set(augmented))

    # 2) Build vocabulary
    stoi, itos = build_vocab(train_smiles)

    # 3) Write reference file if not exists
    if not os.path.exists(REFERENCE_PATH):
        with open(REFERENCE_PATH, "w") as f:
            for s in canon_smiles:
                f.write(s + "\n")

    # 4) Dataset & DataLoader
    dataset = SmilesDataset(train_smiles, stoi, MAX_LEN)
    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)

    # 5) Model & optimizer
    model = VAE(len(itos), EMBED_DIM, HIDDEN_DIM, LATENT_DIM, NUM_LAYERS, stoi).to(DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # 6) Training loop
    for epoch in range(1, NUM_EPOCHS + 1):
        model.train()
        epoch_loss = 0.0
        for x_batch, y_batch in loader:
            x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)
            optimizer.zero_grad()
            recon_logits, mu, logvar = model(x_batch, y_batch, TEACHER_FORCING)
            loss = loss_function(recon_logits, y_batch, mu, logvar)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
            optimizer.step()
            epoch_loss += loss.item()
        avg_loss = epoch_loss / len(loader)
        print(f"Epoch {epoch:02d}/{NUM_EPOCHS}  Loss: {avg_loss:.4f}")

    # 7) Save trained checkpoint
    torch.save(model.state_dict(), CHECKPOINT_PATH)
    print(f"✔ Saved Unconditional VAE to '{CHECKPOINT_PATH}'")

    # 8) Sample a small set to verify
    model.eval()
    with torch.no_grad():
        z = torch.randn(SAMPLE_COUNT, LATENT_DIM, device=DEVICE)
        fake_logits = model.decoder(z, target=None, teacher_forcing_ratio=0.0)
        # Greedy (argmax) sampling
        generated = []
        for i in range(SAMPLE_COUNT):
            seq = ""
            for t in range(fake_logits.size(1)):
                idx = fake_logits[i, t].argmax().item()
                tok = itos[idx]
                if tok == "<eos>":
                    break
                if tok not in ("<pad>", "<bos>"):
                    seq += tok
            generated.append(seq)
    with open(GENERATED_OUT, "w") as f:
        for s in generated:
            f.write(s + "\n")
    print(f"✔ Sampled {len(generated)} random SMILES → '{GENERATED_OUT}'")

if __name__ == "__main__":
    main()
